<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels | yuyu</title><meta name="author" content="yuyu"><meta name="copyright" content="yuyu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本论文在论文分享会中分享，点击查看论文PPT 👆                ⭐论文信息   文献类型：CCFA    发表刊物：AAAI   发表时间：2022   发表单位：中山大学    ⭐Abstract第一句话：介绍本文的研究任务：用部分标注的标签去训练多标签分类模型时一个极具挑战性和实际性的任务。 第二句话：概括现有算法：为了解决这个问题，现有算法主要利用预训练模型">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels">
<meta property="og:url" content="http://example.com/2025/08/24/post6/index.html">
<meta property="og:site_name" content="yuyu">
<meta property="og:description" content="本论文在论文分享会中分享，点击查看论文PPT 👆                ⭐论文信息   文献类型：CCFA    发表刊物：AAAI   发表时间：2022   发表单位：中山大学    ⭐Abstract第一句话：介绍本文的研究任务：用部分标注的标签去训练多标签分类模型时一个极具挑战性和实际性的任务。 第二句话：概括现有算法：为了解决这个问题，现有算法主要利用预训练模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg">
<meta property="article:published_time" content="2025-08-24T06:20:17.000Z">
<meta property="article:modified_time" content="2025-09-11T15:25:54.130Z">
<meta property="article:author" content="yuyu">
<meta property="article:tag" content="标注不完整">
<meta property="article:tag" content="部分标注的多标签分类">
<meta property="article:tag" content="自然图像处理">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels",
  "url": "http://example.com/2025/08/24/post6/",
  "image": "https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg",
  "datePublished": "2025-08-24T06:20:17.000Z",
  "dateModified": "2025-09-11T15:25:54.130Z",
  "author": [
    {
      "@type": "Person",
      "name": "yuyu",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/v.png"><link rel="canonical" href="http://example.com/2025/08/24/post6/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://pic3.zhimg.com/v2-226e577e525f24d3e228737bbd0664a6_r.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/yu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://faculty.ustc.edu.cn/IPlab/zh_CN/more/655343/jsjjgd/index.htm"><i class="fa-fw fas fa-flask"></i><span> 实验室</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/my-daily/"><i class="fa-fw fas fa-calendar-day"></i><span> 我的日常</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://img0.baidu.com/it/u=3750635231,1396466790&amp;fm=253&amp;fmt=auto&amp;app=120&amp;f=JPEG?w=800&amp;h=800" alt="Logo"><span class="site-name">yuyu</span></a><a class="nav-page-title" href="/"><span class="site-name">【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://faculty.ustc.edu.cn/IPlab/zh_CN/more/655343/jsjjgd/index.htm"><i class="fa-fw fas fa-flask"></i><span> 实验室</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/my-daily/"><i class="fa-fw fas fa-calendar-day"></i><span> 我的日常</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-24T06:20:17.000Z" title="发表于 2025-08-24 14:20:17">2025-08-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-11T15:25:54.130Z" title="更新于 2025-09-11 23:25:54">2025-09-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 style="text-align: center; font-size: 24px; font-weight: bold; margin-top: 14px; overflow: hidden; white-space: nowrap;">
  <a href="/sucai/AAAI-2022-类别解耦融合.pdf" style="text-decoration: none; color:rgb(164, 113, 236); display: inline-block; animation: scrollText 10s linear infinite;">
    本论文在论文分享会中分享，点击查看论文PPT 👆
  </a>
</h2>



<figure style="text-align:center;">
    <img src="/sucai/lunwen4/lunwen4.png" style="width: 85%; height: auto;">
</figure>

<h2 id="⭐论文信息"><a href="#⭐论文信息" class="headerlink" title="⭐论文信息"></a>⭐论文信息</h2><div style="display: flex; flex-wrap: wrap;">
  <div style="width: 50%; padding: 10px; font-weight: bold;">文献类型：CCFA </div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表刊物：AAAI</div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表时间：2022</div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表单位：中山大学</div>
</div>


<h2 id="⭐Abstract"><a href="#⭐Abstract" class="headerlink" title="⭐Abstract"></a>⭐Abstract</h2><p><strong>第一句话：介绍本文的研究任务</strong>：用<strong>部分标注</strong>的标签去训练多标签分类模型时一个极具挑战性和实际性的任务。</p>
<p><strong>第二句话：概括现有算法</strong>：为了解决这个问题，现有算法主要利用预训练模型或半监督模型为”unknown label（未发现标签）”生成伪标签。</p>
<p><strong>第三句话：介绍现有方法的局限性</strong>：然而，这些方法需求足够多的”known label（已知标签）”来训练模型，因此在标签稀缺的情况下效果不佳。</p>
<p><strong>第四句话：介绍本文的思路</strong>：为了解决这个问题，本文提出将“在不同图像中将<strong>类别特定（category-specific）的表征</strong>进行融合，将已知标签的信息传播到未知（未发现）标签中作为补充，进而避免对预训练模型的依赖”</p>
<p><strong>第五句话：介绍本文的具体设计</strong>：具体来说，本文提出了一种<strong>语义感知的表征融合（Semantic-Aware Representation Blending, SARB）</strong>方法，通过俩个补充模型分别将<strong>实例级别（instance-level）</strong>和原型级别（prototype-level）的表征进行融合，从而补充未知标签。1）实例级别表征融合（ILRB）：将<strong>一个图像</strong>中已知标签的表征融合到<strong>另外一个图像</strong>相应的未知标签中。2）原型级别表征融合（PLRB）：为每个类别得到一个稳定的原型表征，在未知标签里融合相应的原型表征进行补充。</p>
<p><strong>第六句话：介绍本文的实验结果</strong>：在多个公开数据集上进行的实验表明，本文的方法在各种标签发现率的设定下本文方法都要优于现有主要竞争对手，具体mAP提升了…</p>
<p><strong>第七句话：给出开源代码</strong></p>
<h2 id="⭐Introduction"><a href="#⭐Introduction" class="headerlink" title="⭐Introduction"></a>⭐Introduction</h2><p><strong>段落一：引入部分标注的多标签分类任务的重要性</strong>：多标签图像识别（MLR）旨在从输入图像中提取所有语义标签，相较于单标签任务，MLR是一个更具挑战性和实用性的任务。<strong>由于输入图像和输出标签空间的复杂性，收集具有完整多标签注释的大规模数据集非常耗时。</strong>为了解决这个问题，近年来的研究趋向于研究部分标签的多标签图像识别任务（MLR-PL），在该任务中，仅提供少数正标签和负标签（标注为1或-1），而其他标签未知（见图1，标注为0）。MLR-PL更贴近实际场景，因为它不要求每张图像都拥有完整的多标签注释。</p>
<figure style="text-align:center;">
    <img src="/sucai/lunwen4/f1.png" style="width: 60%; height: auto;">
</figure>

<p><strong>段落二：介绍传统算法和现有算法的缺陷</strong>：<strong>传统算法</strong>：传统算法只是简单地<strong>忽略未知标签</strong>或者<strong>假定其为负标签</strong>，这样都会带来较差的性能，因为它们要么<strong>缺失了标注信息</strong>、要么引入了<strong>错误的标签</strong>。<strong>现有算法</strong>：最近工作提出先用已知标签数据训练出分类模型，并用此模型为未知标签生成伪标签。<strong>现有算法的缺陷（挖坑）</strong>：尽管这些算法取得了令人印象深刻的进展，但它们依赖于足够的多标签注释来进行模型训练，如果将<strong>已知标签比例降低到很小的水平，它们的性能就会明显下降</strong>。</p>
<p><strong>段落三：介绍本文方法思路</strong>：（介绍思路启发）：在n图像中未知的标签c，可能在另外一张m图像是已知存在的，我们可以从m图像中提取标签c的信息，并将其融合到n图像中，这样就为n图像补充了未知的标签c。（单标签场景做法）：先前有工作使用mixup算法将两张<strong>图像融合</strong>，生成一张包含两者语义信息的新图像，从而帮助正则化单标签识别模型的训练。单标签场景算法无法直接运用到多标签场景：然而，<strong>多标签图像通常包含多个语义对象分布在整张图像中，简单地将两张图像混合往往导致语义信息混乱。</strong>（本文SARB框架）：我们设计了一个 <strong>语义感知的表征融合（SARB）框架</strong>，能够学习并融合类别特定的特征表示，用来补全未知标签。<strong>该框架不依赖于预训练模型，并且在已知标签比例不同的情况下都能保持稳定的性能。</strong></p>
<p><strong>段落四：具体介绍本文设计</strong>：具体来说，我们首先引入了一个<strong>类别特定表征学习（CSRL）模块</strong>，该模块利用类别语义来指导生成类别特定的表征。（类别解耦）。设计了一个<strong>实例级表征融合（ILRB）模块</strong>，用于将m图像中已知标签c的表征融合到另一张n图像中对应未知标签的表征上。这样一来，图像n也能包含标签的信息，从而实现标签补全。ILRB能够生成多样化的融合表征以提升性能，但过于多样化的表征可能导致训练不稳定。（<strong>PLRB的动机</strong>）。为了解决这一问题，进一步提出了<strong>原型级表征融合（PLRB）模块</strong>，用于为每个类别学习更鲁棒的表征原型，并将未知标签的表征与对应类别的原型进行融合。这样可以同时生成多样化且稳定的融合表征，用于补全未知标签。</p>
<p><strong>段落五：总结本文贡献</strong>：贡献一：提出了一种语义感知的表征融合框架来补全未知标签，此框架不依赖预训练模型，在所有已知标签比例设置下表现稳定。贡献二：设计了实例级和原型级表征融合模块，可以生成多样且稳定的融合特征表征，以补全未知标签。贡献三：我们在多个大规模多标签识别（MLR）数据集上进行了广泛的实验，包括 Microsoft COCO、Visual Genome以及 Pascal VOC 2007，以验证所提出框架的有效性。我们还进行了消融实验，以深入分析每个模块的实际贡献。 </p>
<h2 id="⭐方法"><a href="#⭐方法" class="headerlink" title="⭐方法"></a>⭐方法</h2><figure style="text-align:center;">
    <img src="/sucai/lunwen4/f2.png" style="width: 80%; height: auto;">
</figure>

<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>给定一张输入图像$I_n$，先用主干网络（backbone）提取全局特征图$f^n$，接着引入 <strong>类别特定表征学习（CSRL）模块 </strong>$\phi_{csrl}$生成类别特定的表征$[f_1^n, f_2^n, …, f_C^n]$，其中C是类别数。然后，设计了<strong>实例级表征融合（ILRB）模块</strong>和<strong>原型级表征融合（PLRB）模块</strong>，分别将类别特定表征进行融合，生成补全未知标签的融合表征。最后，将三种不同表征（原始表征、实例表征融合后、原型表征融合后）输入到分类器$\phi$（GNN+线性分类器+sigmoid）预测出各类别的分数，最后用交叉熵损失函数进行训练。</p>
<h3 id="backbone和CSRL"><a href="#backbone和CSRL" class="headerlink" title="backbone和CSRL"></a>backbone和CSRL</h3><p>类别特定表征学习（CSRL）模块。即根据语义类别从一张多标签图像中提取出对应类别的特征表示，并将其作为类别特定的表征。具体来说，给定一张输入图像$I_n$，先用主干网络（backbone）提取全局特征图$f^n$，接着引入 <strong>类别特定表征学习（CSRL）模块 </strong>$\phi_{csrl}$生成类别特定的表征$[f_1^n, f_2^n, …, f_C^n]$，其中C是类别数。这种设计的初心是：<strong>一张多标签图像散落了多个语义对象在图像的不同位置，直接使用全局特征图往往会混淆不同类别的语义信息</strong>。CSRL模块通过引入类别语义信息来指导类别特定表征的生成，从而实现类别解耦，根据不同类别提取图像中相应的特征。</p>
<p>backbone可以简单的使用一些预训练的卷积神经网络（CNN），如ResNet、VGG等，来提取图像的低级特征。然后，CSRL模块在此基础上进行进一步的特征学习，以获得更具语义信息的类别特定表征。CSRL模块也可以通过不同的现成算法实现，例如<strong>语义解耦(chen et al. 2019b)</strong>或<strong>语义注意力机制(Ye et al. 2020)</strong>等。</p>
<h3 id="实例级表征融合（ILRB）模块"><a href="#实例级表征融合（ILRB）模块" class="headerlink" title="实例级表征融合（ILRB）模块"></a>实例级表征融合（ILRB）模块</h3><p><strong>动机</strong>：直观地说，图像 $I_n$中的未知标签 $c$ 可能在另一幅图像 $I_m$ 中是已知的。ILRB 模块的目的是将图像 $I_m$ 中标签 $c$ 的信息融合到图像 $I_n$ 中，从而使图像 $I_n$ 也能拥有已知标签 $c$。为了实现这一目的，我们融合了属于同一类别的不同图像的表征，将一幅图像的已知标签转移到另一幅图像的未知标签上。</p>
<figure style="text-align:center;">
    <img src="/sucai/lunwen4/f3.png" style="width: 80%; height: auto;">
</figure>

<p>给定俩张训练图像$I_n$和$I_m$，它们分别有：语义特征向量（CSRL后的表征）：$[f_1^n, f_2^n, …, f_C^n]$和$[f_1^m, f_2^m, …, f_C^m]$。其中，$f_i^n$表示图像$I_n$中类别$i$的特征表示，$f_i^m$表示图像$I_m$中类别$i$的特征表示，同样地，它们还有标签向量:$y^n = \{y_1^n, y_2^n, …, y_C^n\}$和$y^m = \{y_1^m, y_2^m, …, y_C^m\}$。接下来ILRB将通过融合俩个图像的语义表征和标签，补全未知标签。</p>
<p>现在假设图像 $I_n$ 中类别 $c$ 的标签是未知的，而图像 $I_m$ 中类别 $c$ 的标签是已知的，即$y_c^n = 0$和$y_c^m = 1$。此时就可以把$m$图像中类别$c$的表征$f_c^m$融合到图像$n$中类别$c$的表征$f_c^n$上，从而生成补全未知标签$c$的融合表征$f_{c}^{n}$。具体来说，其分为<strong>语义表征融合</strong>和<strong>标签融合</strong>俩部分：</p>
<script type="math/tex; mode=display">
\hat{f}_{c}^{n} =
\begin{cases}
\alpha f_c^n + (1-\alpha) f_c^m, & \text{若 } y_c^n = 0 \text{ 且 } y_c^m = 1, \\
f_c^n, & \text{其他情况} 
\end{cases}</script><p><strong>标签融合</strong>如下:</p>
<script type="math/tex; mode=display">
\hat{y}_{c}^{n} =
\begin{cases}
1-\alpha, & \text{若 } y_c^n = 0 \text{ 且 } y_c^m = 1, \\
y_c^n, & \text{其他情况}
\end{cases}</script><p>其中$\alpha \in [0, 1]$是一个超参数，用于控制融合表征中原始表征和补充表征的比例。初始值$\alpha=0.5$。通过这种方式，ILRB模块能够将图像$m$中已知标签$c$的信息有效地传递到图像$n$中，从而实现对未知标签的补全。</p>
<h3 id="原型级表征融合（PLRB）模块"><a href="#原型级表征融合（PLRB）模块" class="headerlink" title="原型级表征融合（PLRB）模块"></a>原型级表征融合（PLRB）模块</h3><p><strong>动机</strong>：虽然 ILRB 模块可以明显提高性能，但它可能会干扰训练过程，因为它会生成许多不同的融合表示进行训练，尤其是当已知标签比例较低时。针对这一问题，我们进一步设计了一个 PLRB 模块，该模块通过学习为每个类别生成更稳定的表示原型，并将图像 $I_n$ 中未知标签的表示与相应类别的原型进行融合。</p>
<figure style="text-align:center;">
    <img src="/sucai/lunwen4/f4.png" style="width: 80%; height: auto;">
</figure>

<p>原型即用于描述每个类别的整体表征（类似于得到一个类别的代表特征），具体地，对于类别$c$，其做法如下：</p>
<p>1、首先收集所有含有已知标签$c$的图像。<br>2、使用CSRL提取这些图像的类别表征，得到向量集合：$[f_c^1, f_c^2, …, f_c^{N_c}]$，其中$N_c$是含有已知标签$c$的图像数量。<br>3、使用<strong>K-means聚类</strong>将这些特征向量聚类为$K$个原型：$P_c = \{p_c^1, p_c^2, …, p_c^K\}$。</p>
<p>接下来为了学习更加稳定的原型，应该设计一个损失：同一类别的表征应该彼此接近，不同类别的表征应该远离。因此，定义对比损失如下：若俩张图像$I_n$和$I_m$都含有已知标签$c$，则它们的类别表征$f_c^n$和$f_c^m$应该彼此接近，即提高<strong>余弦相似度</strong>。否则，减小余弦相似度，因此损失定义如下：</p>
<script type="math/tex; mode=display">
\mathcal{l}_c^{n,m}=
\begin{cases}
1-\cos(f_c^n, f_c^m), & \text{若 } y_c^n = 1 \text{ 且 } y_c^m = 1, \\
1+\cos(f_c^n, f_c^m), & \text{其他情况}
\end{cases}</script><p>总对比损失即：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{cst} =\sum_{n=1}^{N} \sum_{m=1}^{N} \sum_{c=1}^{C} \mathcal{l}_c^{n,m}</script><p><strong>通过上述步骤，即可以学习到一个较为稳定的类别原型</strong>，接下来进行原型级别的融合表征：给定一张输入图像$I_n$，其表征向量为$[f_1^n, f_2^n, …, f_C^n]$，标签向量为$y^n$。先选择一个未知标签$c$（$y_c^n=0$），然后从类别$c$的原型集合$P_c$中随机选择一个原型$p_c^k$，将其融合到图像$I_n$中类别$c$的表征$f_c^n$上，从而生成补全未知标签$c$的融合表征$\tilde{f}_c^n$。具体也分为<strong>表征融合</strong>和<strong>标签融合</strong>俩部分：</p>
<script type="math/tex; mode=display">
\tilde{f}_c^n =
\begin{cases}
\beta f_c^n + (1-\beta) p_c^k, & \text{若 } y_c^n = 0, \\
f_c^n, & \text{其他情况}
\end{cases}</script><script type="math/tex; mode=display">
\tilde{y}_c^n =
\begin{cases}
1-\beta, & \text{若 } y_c^n = 0, \\
y_c^n, & \text{其他情况}
\end{cases}</script><p>其中$\beta \in [0, 1]$是一个超参数，用于控制融合表征中原始表征和补充表征的比例。初始值$\beta=0.5$。通过这种方式，PLRB模块能够将原型信息有效地传递到图像中，从而实现对未知标签的补全，并保证了训练的稳定性。</p>
<h3 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h3><p><figure style="text-align:center;">
    <img src="/sucai/lunwen4/f5.png" style="width: 70%; height: auto;">
</figure><br>综合以上模块，对于一张训练图像$I_n$，经过backbone和CSRL模块，得到类别特定表征$[f_1^n, f_2^n, …, f_C^n]$，对于特定类别$c$，其有三种不同的表征：1）：<strong>原始表征</strong>：$f_c^n$；2）：<strong>实例级融合表征</strong>：$\hat{f}_c^n$；3）：<strong>原型级融合表征</strong>：$\tilde{f}_c^n$。将这三种表征分别输入到分类器$\phi$中，得到对应的预测分数：$s_c^n = \phi(f_c^n)$，$\hat{s}_c^n = \phi(\hat{f}_c^n)$，$\tilde{s}_c^n = \phi(\tilde{f}_c^n)$。（如图所示）然后，使用交叉熵损失函数计算每种表征的分类损失。</p>
<p>按照已有工作，本文采用<strong>部分二元交叉熵损失（partial binary cross entropy loss）</strong> 作为监督网络的目标函数。具体来说，给定预测的概率得分向量$s^n = \{s_1^n, s_2^n, …, s_C^n\}$和标签向量$y^n = \{y_1^n, y_2^n, …, y_C^n\}$，则部分二元交叉熵损失定义如下：</p>
<script type="math/tex; mode=display">
\mathcal{l}(y^n,s^n) = \frac{1}{\sum_{c=1}^{C} |y_c^n|} \sum_{c=1}^{C} \left[ \mathbb{1}_{\{y_c^n = 1\}} \log(s_c^n) + \mathbb{1}_{\{y_c^n = -1\}} \log(1 - s_c^n) \right]</script><p>其中，$\mathbb{1}_{\{y_c^n = 1\}}$是指示函数，当$y_c^n = 1$时取1，否则取0；$\mathbb{1}_{\{y_c^n = -1\}}$同理，<strong>简单来说，部分二元交叉熵损失只考虑正负标签的贡献，而忽略未知标签的影响。</strong></p>
<p>同样地，本文采用该损失监督ILRB和PLRB模块，即$\mathcal{l}(\hat{y}^n, \hat{s}^n)$和$\mathcal{l}(\tilde{y}^n, \tilde{s}^n)$。最终的总损失函数定义如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{cls} = \frac{1}{N} \sum_{n=1}^{N} \left[ \mathcal{l}(y^n, s^n) + \mathcal{l}(\hat{y}^n, \hat{s}^n) + \mathcal{l}(\tilde{y}^n, \tilde{s}^n) \right]</script><p>最后将<strong>分类损失</strong>和<strong>对比损失</strong>结合，获得最终的损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L} = \mathcal{L}_{cls} + \lambda \mathcal{L}_{cst}</script><p>其中，$\lambda$是一个超参数，用于平衡分类损失和对比损失的权重。由于$L_{cst}$通常远大于$L_{cls}$，因此可以将$\lambda$设置为一个较小的值，例如0.05。</p>
<h2 id="⭐Experiments"><a href="#⭐Experiments" class="headerlink" title="⭐Experiments"></a>⭐Experiments</h2><h3 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h3><p>为了公平比较，我们采用 ResNet-101 作为主干网络提取全局特征图，并用在 ImageNet 数据集上预训练的参数初始化，同时随机初始化新增层的参数。训练时，前 91 层 ResNet-101 的参数固定，其他层采用端到端的方式训练。<br>🏋️‍♂️ 训练过程</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>项目</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>优化器</strong></td>
<td>Adam</td>
</tr>
<tr>
<td><strong>批大小</strong></td>
<td>16</td>
</tr>
<tr>
<td><strong>动量</strong></td>
<td>0.999 和 0.9</td>
</tr>
<tr>
<td><strong>权重衰减</strong></td>
<td>$5 \times 10^{-4}$</td>
</tr>
<tr>
<td><strong>初始学习率</strong></td>
<td>$1 \times 10^{-5}$（每 10 个 epoch 降低 10 倍）</td>
</tr>
<tr>
<td><strong>总轮数</strong></td>
<td>20 epoch</td>
</tr>
</tbody>
</table>
</div>
<p>📈 数据增强</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>输入图像缩放到 $512 \times 512$</td>
</tr>
<tr>
<td>2</td>
<td>随机选择 $\{512, 448, 384, 320, 256\}$ 作为裁剪大小</td>
</tr>
<tr>
<td>3</td>
<td>裁剪后再缩放到 $448 \times 448$</td>
</tr>
<tr>
<td>4</td>
<td>随机水平翻转</td>
</tr>
</tbody>
</table>
</div>
<p>⚙️ 训练策略</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间点</th>
<th>策略</th>
</tr>
</thead>
<tbody>
<tr>
<td>从第 5 个 epoch 开始</td>
<td>使用 <strong>ILRB</strong> 和 <strong>PLRB</strong> 模块</td>
</tr>
<tr>
<td>每隔 5 个 epoch</td>
<td>重新计算各类别的原型</td>
</tr>
</tbody>
</table>
</div>
<p>🔎 推理阶段</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>项目</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模块</td>
<td>移除 <strong>ILRB</strong> 和 <strong>PLRB</strong></td>
</tr>
<tr>
<td>输入</td>
<td>图像统一调整为 $448 \times 448$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在 MS-COCO (Lin et al. 2014)、Visual Genome (Krishna et al. 2016) 和 Pascal VOC 2007 (Everingham et al. 2010) 上进行实验，用于公平对比。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>类别数/描述</th>
<th>训练集</th>
<th>验证/测试集</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>MS-COCO</td>
<td>80 个日常生活类别</td>
<td>82,801 张图像</td>
<td>40,504 张图像</td>
<td></td>
</tr>
<tr>
<td>Pascal VOC 2007</td>
<td>20 个类别</td>
<td>-</td>
<td>9,963 张图像</td>
<td></td>
</tr>
<tr>
<td>Visual Genome</td>
<td>80,138 个类别</td>
<td>98,249 张图像</td>
<td>10,000 张图像</td>
<td><strong>选取 200 个最常见类别构建 VG-200 子集</strong></td>
</tr>
<tr>
<td>VG-200</td>
<td>200 个类别</td>
<td></td>
<td></td>
<td>该划分将公开供进一步研究</td>
</tr>
</tbody>
</table>
</div>
<h3 id="实验设定"><a href="#实验设定" class="headerlink" title="实验设定"></a>实验设定</h3><p>所有的数据集均具有完整标注。为了模拟部分标注，参考前人工作 (Durand et al. 2019; Huynh &amp; Elhamifar 2020)：<strong>随机丢弃部分正负标签，制造“部分标注”数据集</strong>。在这项工作中，丢弃标签的比例从 90% 到 10% ，因此已知标签的比例为 10% 到 90%。为了进行公平的比较，我们采用了所有类别的<strong>平均精度（mAP）</strong>来评估不同比例的已知标签。为了进行更全面的评估，我们还计算了所有比例的平均 mAP。此外，我们还遵循之前大多数 MLR 作品（Chen 等人，2019b）的做法，采用整体（overall）和每类（per）精度、召回率、F1-measure（<strong>即 OP、OR、OF1、CP、CR 和 CF1</strong>）进行更全面的评估。</p>
<h3 id="SOTA对比实验系列"><a href="#SOTA对比实验系列" class="headerlink" title="SOTA对比实验系列"></a>SOTA对比实验系列</h3><p>为了评估所提出的 SARB 框架的有效性，我们将其与传统的 MLR 算法和当前的 MLR-PL 算法进行了比较。</p>
<p align="center">
  <img src="/sucai/lunwen4/f6.png" width="80%">
</p>


<p>由于MLR算法是无法直接处理未知标签数据的，这里作者统一将MLR算法中的损失修改成<strong>部分二元交叉熵损失</strong>以进行对比。</p>
<h3 id="COCO数据集上表现"><a href="#COCO数据集上表现" class="headerlink" title="COCO数据集上表现"></a>COCO数据集上表现</h3><p align="center">
  <img src="/sucai/lunwen4/f7.png" width="80%">
</p>

<p align="center">
  <img src="/sucai/lunwen4/f8.png" width="50%">
</p>

<p>与目前最先进的算法相比，我们的 SARB 框架获得了<strong>最佳性能</strong>。（如表+图）<br>（列数据）如表所示，该算法的 mAP、OF1 和 CF1 平均值分别为 77.9%、76.5% 和 72.2%，比之前表现最好的 KGGR 算法分别高出 2.3%、2.8% 和 2.5%。如图所示，<strong>在所有已知标签比例设置中，SARB 框架也能获得更好的 mAP。</strong><br>值得注意的是，当已知标签比例降低时，SARB 框架的性能提升更为明显。（如图）例如，当使用 90% 和 10% 的已知标签时，mAP 与之前的最佳 KGGR 算法相比分别提高了 1.4% 和 4.6%。<br>（摆结论）这些比较表明，<strong>SARB 框架可以适应不同的比例设置，因为它不依赖于预先训练的模型</strong>。（回应挖坑）</p>
<h3 id="VG-200数据集上表现"><a href="#VG-200数据集上表现" class="headerlink" title="VG-200数据集上表现"></a>VG-200数据集上表现</h3><p align="center">
  <img src="/sucai/lunwen4/f9.png" width="80%">
</p>

<p align="center">
  <img src="/sucai/lunwen4/f10.png" width="50%">
</p>

<p>（数据背景+找补）VG200 是一个<strong>更具挑战性的基准</strong>，涵盖的类别更多。因此，目前的工作性能相当差。<br>（列数据）如表，之前性能最好的 KGGR 算法的 mAP、OF1 和 CF1 平均值分别为 41.5%、41.2% 和 33.6%。在这种情况下，我们的 SARB 框架表现出<strong>更明显的性能提升</strong>。其平均 mAP、OF1 和 CF1 分别为 45.6%、45.0% 和 37.4%，比 KGGR 算法分别高出 4.1%、3.8% 和 3.8%。<br>（列数据）如图，与现有算法相比，我们发现我们的框架在所有已知标签比例设置下的 mAP 提高了 3.3% 以上。</p>
<h3 id="VOC数据集上表现"><a href="#VOC数据集上表现" class="headerlink" title="VOC数据集上表现"></a>VOC数据集上表现</h3><p align="center">
  <img src="/sucai/lunwen4/f11.png" width="80%">
</p>

<p align="center">
  <img src="/sucai/lunwen4/f12.png" width="50%">
</p>

<p>（数据背景）Pascal VOC 2007 是评估多标签图像识别最广泛使用的数据集。由于该数据集仅涵盖 20 个类别，是一个简单得多的数据集，目前的算法也能取得相当不错的性能。<br>（列数据）如表 1 所示，之前性能最好的 KGGR 算法的 mAP、OF1 和 CF1 平均值分别为 41.5%、41.2% 和 33.6%。在这种情况下，我们的 SARB 框架表现出<strong>更明显的性能提升</strong>。它的平均 mAP、OF1 和 CF1 分别为 45.6%、45.0% 和 37.4%，比 KGGR 算法分别高出 4.1%、3.8% 和 3.8%。<br>（列数据）现有算法相比，我们发现我们的框架在所有已知标签比例设置下的 mAP 提高了 3.3%。</p>
<h3 id="消融实验-CSRL"><a href="#消融实验-CSRL" class="headerlink" title="消融实验-CSRL"></a>消融实验-CSRL</h3><p align="center">
  <img src="/sucai/lunwen4/f13.png" width="50%">
</p>

<p>（CSRL方法）CSRL 模块用于提取类别特定的特征表征，可采用不同算法实现：<strong>语义解耦 (SD)</strong> (Chen et al.)<strong>语义注意机制 (SAM)</strong> (Ye et al. 2020)。<br>（基线方法）传统 Mixup 算法通过 位置级融合生成新样本，增强训练。本文实现了两个基线算法：IP-Mixup：在图像空间做融合，FM-Mixup：在特征空间做融合。（不解耦，直接融合）<br>（结论）<strong>SD 和 SAM 性能接近， SD 略优于 SAM</strong>，因此后续实验全部采用 SD 实现 CSRL 模块。<br>（结论）这两种 Mixup 与 SSGRL 基线性能相近，<strong>因为简单融合无法带来额外信息</strong>。<br>（结论）与 基于 CSRL 的 SARB 相比：<br>IP-Mixup 在三个数据集上 mAP 分别下降 3.6%、5.9%、1.0%<br>FM-Mixup 在三个数据集上 mAP 分别下降 3.8%、6.0%、1.1%</p>
<h3 id="消融实验-ILRB"><a href="#消融实验-ILRB" class="headerlink" title="消融实验-ILRB"></a>消融实验-ILRB</h3><p align="center">
  <img src="/sucai/lunwen4/f14.png" width="50%">
</p>

<p>为了分析 ILRB 模块 的实际贡献，作者进行了只使用该模块的实验（称为 Ours ILRB），并与 SSGRL 基线在 MS-COCO、VG-200 和 Pascal VOC 2007 三个数据集上进行了对比。<br>ILRB 模块包含一个关键参数 $\alpha$，用于控制 实例级融合的比例。为了验证 $\alpha$ 的可学习性带来的贡献，作者对比了使用固定值 $\alpha=0.5$ 的情况。<br>（结论）实验表明，<strong>Ours ILRB在各项数据表现均优于SSGRL</strong>，mAP分别提升了3.2、5.2、0.7。<br>（结论）实验表明， Ours ILRB在各项数据表现均优于Ours ILRB fixed $\alpha$。说明$\alpha$ <strong>固定会降低性能</strong>，而 自适应 $\alpha$ 更优。</p>
<h3 id="消融实验-PLRB"><a href="#消融实验-PLRB" class="headerlink" title="消融实验-PLRB"></a>消融实验-PLRB</h3><p align="center">
  <img src="/sucai/lunwen4/f15.png" width="50%">
</p>

<p align="center">
  <img src="/sucai/lunwen4/f16.png" width="50%">
</p>

<p>同样地，PLRB 模块也是框架中的关键部分。为了分析其有效性，作者使用了只使用PLRB的实验，并与基线SSGRL对比。同时也针对可学习参数$\beta$进行的实验。<br>（结论）加入 PLRB 后，相较于SSGRL，mAP分别提升了3.2、5.2、0.9。<br>（结论）在 训练损失曲线可视化（图 4） 中可以看到：没有 PLRB → loss 波动明显（不稳定），加入 PLRB → loss 曲线更加平滑（训练稳定），<strong>根据之前的分析，PLRB 模块有助于生成稳定的融合表示，从而补充未知标签并使训练更加稳定</strong>。<br>（结论）自适应 $\beta$ 优于固定 $\beta$。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项工作中，我们提出了一种新的视角：通过<strong>融合类别特定的表征</strong>来补充未知标签，从而解决 MLR-PL 任务。这种方法不依赖于充分的标注，因此在所有已知标签比例的设置下都能获得更优的性能。具体来说，我们的方法包含两个核心模块：<strong>ILRB 模块</strong>：融合已知标签的 实例级表示，以补充对应未知标签的表示；<strong>PLRB 模块</strong>：学习并融合 原型级表示，以补充对应未知标签的表示。这两个模块能够同时生成<strong>多样化且稳定的融合表示来弥补未知标签</strong>，从而促进 MLR-PL 任务的完成。在 MS-COCO、VG-200 和 Pascal VOC 数据集上的大量实验验证了该方法相较于现有算法的优越性。</p>
<style>
    .highlight-text {
      background-color: #f0f8ff;         /* 设置背景色 */
      border-radius: 4px;                /* 设置圆角 */
      padding: 0 5px;                    /* 设置内边距，确保文本不与边框贴得太紧 */
      box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);  /* 添加轻微的阴影 */
      border: 2px solidrgb(0, 98, 133);         /* 添加边缘线，颜色为浅灰色，宽度为 2px */
    }
  </style>

  <style>
    .highlight-normal {
      background-color: #fff8dc;         /* 设置浅黄色背景色 */
      border-radius: 4px;                /* 设置圆角 */
      padding: 0 5px;                    /* 设置内边距，确保文本不与边框贴得太紧 */
      box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);  /* 添加轻微的阴影 */
      border: 1px solid #f0e68c;         /* 添加浅黄色边缘线，颜色为浅黄色，宽度为 2px */
    }
  </style>

  <style>
    .bbox {
      background-color: #f0f8ff;        /* 设置背景色为浅黄色 */
      border-radius: 8px;               /* 设置圆角 */
      padding: 10px;                    /* 设置内边距 */
      box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* 添加轻微的阴影 */
      border: 2px solidrgb(126, 230, 206);        /* 设置浅黄色边框 */
      display: block;                   /* 确保框体显示为块元素，独占一行 */
      margin-top: 10px;                 /* 给框体添加顶部间距 */
      font-family: "Times New Roman", Times, serif; /* 设置字体为 Times New Roman */
    }
  </style>


  <style>
    .yubox {
      background-color:rgb(252, 252, 203);        /* 设置背景色为浅黄色 */
      border-radius: 8px;               /* 设置圆角 */
      padding: 10px;                    /* 设置内边距 */
      box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* 添加轻微的阴影 */
      border: 2px solidrgb(126, 230, 206);        /* 设置浅黄色边框 */
      display: block;                   /* 确保框体显示为块元素，独占一行 */
      margin-top: 10px;                 /* 给框体添加顶部间距 */
      font-family: "Times New Roman", Times, serif; /* 设置字体为 Times New Roman */
    }
  </style></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">yuyu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/08/24/post6/">http://example.com/2025/08/24/post6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">yuyu</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%A0%87%E6%B3%A8%E4%B8%8D%E5%AE%8C%E6%95%B4/">标注不完整</a><a class="post-meta__tags" href="/tags/%E9%83%A8%E5%88%86%E6%A0%87%E6%B3%A8%E7%9A%84%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/">部分标注的多标签分类</a><a class="post-meta__tags" href="/tags/%E8%87%AA%E7%84%B6%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">自然图像处理</a></div><div class="post-share"><div class="social-share" data-image="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/21/post5/" title="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model"><img class="cover" src="https://picx.zhimg.com/v2-31af717d5ebecb7d19dd56b65383af7f_1440w.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model</div></div><div class="info-2"><div class="info-item-1">        ⭐论文信息   文献类型：预印本 arxiv    发表刊物：arxiv   发表时间：2024   发表单位：清华大学   1.1 拟解决的科学问题✨ 本论文旨在构建一个适用于3D医学影像理解的多模态大模型，主要解决三维特征提取问题。✨ 本文属于医学大模型、多模态大模型、3D特征提取、局部全局特征融合等领域。 ⭐论文背景2.1...</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/yu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yuyu</div><div class="author-info-description">欢迎来到yuyu的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/gityuyuhub" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:2743342512@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如你所见，这是我的博客，相信在这些文章中，你会找到你想要的。This is Yuyu</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">
  
    本论文在论文分享会中分享，点击查看论文PPT 👆
  
</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E8%AE%BA%E6%96%87%E4%BF%A1%E6%81%AF"><span class="toc-number">2.</span> <span class="toc-text">⭐论文信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90Abstract"><span class="toc-number">3.</span> <span class="toc-text">⭐Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90Introduction"><span class="toc-number">4.</span> <span class="toc-text">⭐Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">⭐方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">5.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#backbone%E5%92%8CCSRL"><span class="toc-number">5.2.</span> <span class="toc-text">backbone和CSRL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E7%BA%A7%E8%A1%A8%E5%BE%81%E8%9E%8D%E5%90%88%EF%BC%88ILRB%EF%BC%89%E6%A8%A1%E5%9D%97"><span class="toc-number">5.3.</span> <span class="toc-text">实例级表征融合（ILRB）模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9E%8B%E7%BA%A7%E8%A1%A8%E5%BE%81%E8%9E%8D%E5%90%88%EF%BC%88PLRB%EF%BC%89%E6%A8%A1%E5%9D%97"><span class="toc-number">5.4.</span> <span class="toc-text">原型级表征融合（PLRB）模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88"><span class="toc-number">5.5.</span> <span class="toc-text">优化方案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90Experiments"><span class="toc-number">6.</span> <span class="toc-text">⭐Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82"><span class="toc-number">6.1.</span> <span class="toc-text">实施细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.2.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A"><span class="toc-number">6.3.</span> <span class="toc-text">实验设定</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SOTA%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C%E7%B3%BB%E5%88%97"><span class="toc-number">6.4.</span> <span class="toc-text">SOTA对比实验系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COCO%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%A1%A8%E7%8E%B0"><span class="toc-number">6.5.</span> <span class="toc-text">COCO数据集上表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VG-200%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%A1%A8%E7%8E%B0"><span class="toc-number">6.6.</span> <span class="toc-text">VG-200数据集上表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VOC%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%A1%A8%E7%8E%B0"><span class="toc-number">6.7.</span> <span class="toc-text">VOC数据集上表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-CSRL"><span class="toc-number">6.8.</span> <span class="toc-text">消融实验-CSRL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-ILRB"><span class="toc-number">6.9.</span> <span class="toc-text">消融实验-ILRB</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C-PLRB"><span class="toc-number">6.10.</span> <span class="toc-text">消融实验-PLRB</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/24/post6/" title="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels"><img src="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels"></a><div class="content"><a class="title" href="/2025/08/24/post6/" title="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels">【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels</a><time datetime="2025-08-24T06:20:17.000Z" title="发表于 2025-08-24 14:20:17">2025-08-24</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/24/post6/" title="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels"><img src="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels"/></a><div class="content"><a class="title" href="/2025/08/24/post6/" title="【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels">【论文阅读】：Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels</a><time datetime="2025-08-24T06:20:17.000Z" title="发表于 2025-08-24 14:20:17">2025-08-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/21/post5/" title="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model"><img src="https://picx.zhimg.com/v2-31af717d5ebecb7d19dd56b65383af7f_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model"/></a><div class="content"><a class="title" href="/2025/04/21/post5/" title="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model">【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model</a><time datetime="2025-04-20T17:57:17.000Z" title="发表于 2025-04-21 01:57:17">2025-04-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/16/post4/" title="【技术篇】BCE or CE? 多标签 还是 多分类？"><img src="https://picx.zhimg.com/v2-701cefaa24bd17aa93e71ae007e23f03_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【技术篇】BCE or CE? 多标签 还是 多分类？"/></a><div class="content"><a class="title" href="/2025/03/16/post4/" title="【技术篇】BCE or CE? 多标签 还是 多分类？">【技术篇】BCE or CE? 多标签 还是 多分类？</a><time datetime="2025-03-15T16:54:00.000Z" title="发表于 2025-03-16 00:54:00">2025-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/post3/" title="【技术篇】transformer解析"><img src="https://pic1.zhimg.com/v2-ffa731613043c6236cf6fd5dd003aa46_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【技术篇】transformer解析"/></a><div class="content"><a class="title" href="/2025/03/04/post3/" title="【技术篇】transformer解析">【技术篇】transformer解析</a><time datetime="2025-03-03T16:58:17.000Z" title="发表于 2025-03-04 00:58:17">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/post2/" title="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification"><img src="https://pica.zhimg.com/v2-fa99c36ab8d889eb688fd3b67b4fc448_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification"/></a><div class="content"><a class="title" href="/2025/02/28/post2/" title="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification">【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification</a><time datetime="2025-02-27T16:49:17.000Z" title="发表于 2025-02-28 00:49:17">2025-02-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By yuyu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23lienmr7axdYseqeC',
      clientSecret: 'b7f1b4e555108e1113715eb05f7527c6d1ad667d',
      repo: 'gityuyuhub.github.io',
      owner: 'gityuyuhub',
      admin: ['gityuyuhub'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || '6edbef9f90867f26b1c83fe46a8a24a9'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>