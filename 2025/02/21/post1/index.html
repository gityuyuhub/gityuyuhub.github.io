<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification | yuyu</title><meta name="author" content="yuyu"><meta name="copyright" content="yuyu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本论文在论文分享会中分享，点击查看论文PPT 👆             ⭐论文信息   文献类型：会议论文(CCFA)   发表刊物：CVPR   发表时间：2022   发表单位：首尔国立大学   1.1 拟解决的科学问题✨ 本论文旨在解决多标签数据集中标签部分遗漏的问题。✨ 本文属于 弱监督学习 领域。具体属于 弱标签学习 领域或 弱监督多标签学习（WSML） 领域。 ⭐论">
<meta property="og:type" content="article">
<meta property="og:title" content="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification">
<meta property="og:url" content="http://example.com/2025/02/21/post1/index.html">
<meta property="og:site_name" content="yuyu">
<meta property="og:description" content="本论文在论文分享会中分享，点击查看论文PPT 👆             ⭐论文信息   文献类型：会议论文(CCFA)   发表刊物：CVPR   发表时间：2022   发表单位：首尔国立大学   1.1 拟解决的科学问题✨ 本论文旨在解决多标签数据集中标签部分遗漏的问题。✨ 本文属于 弱监督学习 领域。具体属于 弱标签学习 领域或 弱监督多标签学习（WSML） 领域。 ⭐论">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg">
<meta property="article:published_time" content="2025-02-20T16:54:42.000Z">
<meta property="article:modified_time" content="2025-03-25T16:59:04.508Z">
<meta property="article:author" content="yuyu">
<meta property="article:tag" content="弱监督学习">
<meta property="article:tag" content="弱标签学习">
<meta property="article:tag" content="多标签学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification",
  "url": "http://example.com/2025/02/21/post1/",
  "image": "https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg",
  "datePublished": "2025-02-20T16:54:42.000Z",
  "dateModified": "2025-03-25T16:59:04.508Z",
  "author": [
    {
      "@type": "Person",
      "name": "yuyu",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/v.png"><link rel="canonical" href="http://example.com/2025/02/21/post1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://pic3.zhimg.com/v2-226e577e525f24d3e228737bbd0664a6_r.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/yu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://faculty.ustc.edu.cn/IPlab/zh_CN/more/655343/jsjjgd/index.htm"><i class="fa-fw fas fa-flask"></i><span> 实验室</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/my-daily/"><i class="fa-fw fas fa-calendar-day"></i><span> 我的日常</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://img0.baidu.com/it/u=3750635231,1396466790&amp;fm=253&amp;fmt=auto&amp;app=120&amp;f=JPEG?w=800&amp;h=800" alt="Logo"><span class="site-name">yuyu</span></a><a class="nav-page-title" href="/"><span class="site-name">【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://faculty.ustc.edu.cn/IPlab/zh_CN/more/655343/jsjjgd/index.htm"><i class="fa-fw fas fa-flask"></i><span> 实验室</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/my-daily/"><i class="fa-fw fas fa-calendar-day"></i><span> 我的日常</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-20T16:54:42.000Z" title="发表于 2025-02-21 00:54:42">2025-02-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-25T16:59:04.508Z" title="更新于 2025-03-26 00:59:04">2025-03-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 style="text-align: center; font-size: 24px; font-weight: bold; margin-top: 14px; overflow: hidden; white-space: nowrap;">
  <a href="/sucai/20250226-论文分享会.pdf" style="text-decoration: none; color:rgb(164, 113, 236); display: inline-block; animation: scrollText 10s linear infinite;">
    本论文在论文分享会中分享，点击查看论文PPT 👆
  </a>
</h2>







<p><img src="/sucai/lunwen1.png" alt="论文信息"></p>
<h2 id="⭐论文信息"><a href="#⭐论文信息" class="headerlink" title="⭐论文信息"></a>⭐论文信息</h2><div style="display: flex; flex-wrap: wrap;">
  <div style="width: 50%; padding: 10px; font-weight: bold;">文献类型：会议论文(CCFA)</div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表刊物：CVPR</div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表时间：2022</div>
  <div style="width: 50%; padding: 10px; font-weight: bold;">发表单位：首尔国立大学</div>
</div>

<h3 id="1-1-拟解决的科学问题"><a href="#1-1-拟解决的科学问题" class="headerlink" title="1.1 拟解决的科学问题"></a>1.1 拟解决的科学问题</h3><p>✨ 本论文旨在解决多标签数据集中<span style="color: red;"><strong>标签部分遗漏</strong></span>的问题。<br>✨ 本文属于 <span class="highlight-text"><strong>弱监督学习</strong></span> 领域。具体属于 <span class="highlight-text"><strong>弱标签学习</strong></span> 领域或 <span class="highlight-text"><strong>弱监督多标签学习（WSML）</strong></span> 领域。</p>
<h2 id="⭐论文背景"><a href="#⭐论文背景" class="headerlink" title="⭐论文背景"></a>⭐论文背景</h2><h3 id="2-1-基本背景"><a href="#2-1-基本背景" class="headerlink" title="2.1 基本背景"></a>2.1 基本背景</h3><p>✨ <strong>弱监督多标签学习（WSML）</strong><br><span class="bbox">In a WSML setting, labels are given as a form of partial label, which means only a small amount of categories is annotated per image. This setting reflects the recently released large-scale multi-label datasets [12,19] which provide only partial label.
</span></p>
<p>✨ <strong>传统解决方案流派</strong>：<br>第一种是：忽略<span class="highlight-normal"><strong>部分缺失标签</strong></span>，直接拿已有标签进行训练。<br>第二种是：直接把<span class="highlight-normal"><strong>部分缺失标签</strong></span>判定为负样本，进行训练。<br><span class="bbox">There are two naive approaches to train the model with partial labels. One is to train the model with observed labels only, ignoring the unobserved labels. The other is to assume all unobserved labels are negative and incorporate them into training because majorities of labels are negative in a multilabel setting [32].
</span></p>
<p>✨ <strong>记忆效应</strong>：模型在训练时会首先记住<span color="red"><strong>干净标签</strong></span>，而之后再学习<span color="red"><strong>噪声标签</strong></span>。（如图所示，干净标签的样本损失一直下降，而噪声标签的样本损失则<strong>先上升后下降</strong>）</p>
<p><div style="text-align:center;">
  <img src="/sucai/l1_f1.png" height="250">
</div> <span class="bbox">Our key observation is about the memorization effect [1] in a noisy label learning literature. It is known that when training a model with a noisy label, the model fits into clean labels first and then starts memorizing noisy labels. Although previous work showed the memorization effect only in a noisy multi-class classification scenario, we found for the first time that this same effect also happens in a noisy multi-label classification scenario.
</span></p>
<h3 id="2-2-挖坑"><a href="#2-2-挖坑" class="headerlink" title="2.2 挖坑"></a>2.2 挖坑</h3><p>✨ 现有数据集中一个<span class="highlight-normal"><strong>实例</strong></span>实际上可能有多个标签，然而多标签的数据标注<span style="color: red;">费时费力且难度大</span>，影响了模型的性能。<br><span class="bbox">However, the multi-label classification task has some fundamental difficulties in making a dataset because it requires annotators to label all categories’ existence/absence for every image. As the number of categories and images in the dataset increase, annotation cost becomes tremendous [19].
</span></p>
<p>✨ 第一种解决流派的前沿技术具有算力要求高、优化更复杂的缺陷;而第二种解决流派则将带噪的标签引入模型，影响模型的学习。<br><span class="bbox">As the second one has a limitation that this assumption produces some noise in a label which hampers the model learning, previous works [7,9,16,21] mostly follow the first approach and try to explore the cue of unobserved labels using various techniques such as bootstrapping or regularization. However, these approaches include heavy computation or complex optimization pipeline.
</span></p>
<h3 id="2-3-一段话介绍技术"><a href="#2-3-一段话介绍技术" class="headerlink" title="2.3 一段话介绍技术"></a>2.3 一段话介绍技术</h3><p>基于这一发现，我们借鉴了<strong>噪声多类别</strong>领域文献中的思想[13, 17, 23]，即通过选择性地<strong>训练具有小损失</strong>的样本，并将这一思想应用于多标签情境。具体来说，通过在WSML设置中将<strong>未知标签视为负标签</strong>，标签噪声以假阴性的形式出现。然后，我们开发了三种不同的方案，通过在训练过程中<strong>拒绝或修正大损失样本</strong>，防止假阴性标签被记忆到多标签分类模型中。<br><span class="bbox">Based on this finding, we borrow the idea from noisy multi-class literature [13, 17, 23] which selectively trains the model with samples having small loss and adapt this idea into a multi-label scenario. Specifically, by assigning the unknown labels as negative in a WSML setting, label noise appears in the form of false negative. Then we develop the three different schemes to prevent false negative labels from being memorized into the multi-label classification model by rejecting or correcting large loss samples during training.
</span></p>
<h3 id="2-4-相关工作"><a href="#2-4-相关工作" class="headerlink" title="2.4 相关工作"></a>2.4 相关工作</h3><p>主要可以把相关工作分为三类：<span class="highlight-text"><strong>多标签学习</strong></span>、<span class="highlight-text"><strong>弱监督多标签分类</strong></span>、<span class="highlight-text"><strong>带噪多分类</strong></span>。他们的工作可以列举如下表格：  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">研究领域/研究方法</th>
<th style="text-align:center">流派1</th>
<th style="text-align:center">流派2</th>
<th style="text-align:center">流派3</th>
<th style="text-align:center">流派4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>多标签学习</strong></td>
<td style="text-align:center">研究标签相关性</td>
<td style="text-align:center">研究数据不平衡</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>弱监督多标签分类 WSML</strong></td>
<td style="text-align:center">忽略缺失标签方法训练</td>
<td style="text-align:center">借助标签相关性和图像相关性预测缺失标签</td>
<td style="text-align:center">训练时，只选择一个正标签学习，使用正则化优化分类器和缺失标签预测</td>
<td style="text-align:center">直接把缺失标签判定为负样本</td>
</tr>
<tr>
<td style="text-align:center"><strong>带噪多分类</strong></td>
<td style="text-align:center">样本筛选策略</td>
<td style="text-align:center">标签修正策略</td>
<td style="text-align:center">样本筛选 + 标签修正</td>
</tr>
</tbody>
</table>
</div>
<h2 id="⭐论文方法"><a href="#⭐论文方法" class="headerlink" title="⭐论文方法"></a>⭐论文方法</h2><p><img src="/sucai/l1_f2.png" alt="alt text"></p>
<p>  对于一个样本$\boldsymbol{x}$，它的标注的标签为 Targrt $\boldsymbol{y}$，我们首先假定$\boldsymbol{y}$中所有<strong>缺失的标签</strong>都是负标签进而获得$\boldsymbol{y}^{AN}$。然后我们通过一个预测器$\boldsymbol{f}$来预测$\boldsymbol{y}^{AN}$，得到$g(\boldsymbol{x})$,之后计算它们的交叉熵$\boldsymbol{l}=\text{BCE}{(f(\boldsymbol{x}),\boldsymbol{y}^{AN})}$。接着我们通过三种修正方法来修正$\boldsymbol{l}$，分别是<span class="highlight-text"><strong>大损失标签拒绝(LL-R)</strong></span>、<span class="highlight-text"><strong>大损失标签临时修正(LL-Ct)</strong></span>、<span class="highlight-text"><strong>大损失标签修正(LL-Cp)</strong></span>。假设有$N$个训练样本，每个样本有$K$个标签。</p>
<h3 id="3-1-大损失标签拒绝-LL-R"><a href="#3-1-大损失标签拒绝-LL-R" class="headerlink" title="3.1 大损失标签拒绝(LL-R)"></a>3.1 大损失标签拒绝(LL-R)</h3><p>  该方法的核心思想是拒绝大损失标签（<strong>注意是拒绝标签，不是拒绝样本</strong>），即当$\boldsymbol{l}$大于阈值$R(t)$时，我们拒绝该标签的梯度更新。这样可以防止模型记忆噪声标签，其中$R(t)$的设定<span color="red">有点令人晕晕的</span>；而防止模型记忆噪声标签的设计是通过修正$\boldsymbol{l}$来实现的，具体是设计了<strong>损失权值</strong>$\lambda$，使得$\boldsymbol{l}$拒绝较大的损失标签。</p>
<h4 id="阈值设计"><a href="#阈值设计" class="headerlink" title="阈值设计"></a>阈值设计</h4><p>  LL-R中阈值$R(t)$是一个随着<strong>训练轮次</strong>$t$逐渐增大的函数，其设计如下：</p>
<script type="math/tex; mode=display">
  R(t)= [(t-1)\cdot \Delta_{rel}] \% \  \text{of} \ {\boldsymbol{l}_{\text{max，缺失标签}}}</script><p>  其中$\boldsymbol{l}_{\text{max}}$是当前训练轮次中的最大损失，例如当$[(t-1)\cdot \Delta_{rel}] \%=0.8$时，$R(t)$将当前轮次损失前$80\%$大的损失（不是最大值乘以0.8，假定这轮每一个样本有100个样本，每一个样本有2个标签缺失，则总共有200个标签缺失，则这个阈值即按算是大到小排列第160个缺失标签的BCE损失）作为阈值。这种设计导致随着训练轮次的增加，阈值$R(t)$的百分比逐渐增大，从而拒绝<strong>更多</strong>的大损失标签。其中$\Delta_{rel}$是一个超参数，用于控制阈值的增长速度。应该注意到，当$t=1$时，$R(t)=0$，即第一轮 (<span style="color: red;">预热阶段</span>)不会拒绝任何标签。<br><span class="yubox">我觉得这种设计令人费解，为什么阈值按照百分比进行设定，这种设计导致每一轮<strong>必须淘汰掉一定比例的标签</strong>，这样的设计是否合理？为什么不直接设定一个损失阈值呢，甚至直接拿本轮损失的最大值乘以一个系数作为阈值？
</span></p>
<h4 id="损失权值设计"><a href="#损失权值设计" class="headerlink" title="损失权值设计"></a>损失权值设计</h4><p>  为了<strong>拒绝</strong>大损失标签，LL-R设计了一个损失权值$\lambda$，使得$\boldsymbol{l}$拒绝较大的损失标签。具体来说，我们通过以下公式来计算$\lambda$：</p>
<script type="math/tex; mode=display">
  \lambda_i = \begin{cases}
  0, & \text{if} \ \boldsymbol{l}_i > R(t)\text{且该标签是缺失标签} \\
  1，& \text{otherwise}  \end{cases}</script><p> 这里$\boldsymbol{l}_i$是样本$\boldsymbol{x}$的第$i$个标签的损失，$R(t)$是阈值，$\lambda_i$是损失权值。这样，当$\boldsymbol{l}_i$大于阈值$R(t)$且该标签是缺失标签时，我们将$\lambda_i$设为0，即拒绝该标签的梯度更新。这样，我们可以防止模型记忆噪声标签；最终计算的损失为：</p>
<script type="math/tex; mode=display">
  \boldsymbol{L} = \frac{1}{N} \sum_{\boldsymbol{x,y^{\text{AN}}}}^{N} \sum_{i=1}^{K} \frac{1}{K} \lambda_i \cdot \boldsymbol{l}_i</script><p>  这样，我们可以通过拒绝大损失标签来防止模型记忆噪声标签。</p>
<h3 id="3-2-大损失标签临时修正-LL-Ct"><a href="#3-2-大损失标签临时修正-LL-Ct" class="headerlink" title="3.2 大损失标签临时修正(LL-Ct)"></a>3.2 大损失标签临时修正(LL-Ct)</h3><p>  该方法的核心思想是在训练过程中，临时修正大损失标签，即当$\boldsymbol{L}$大于阈值$R(t)$时，我们临时<strong>修正该标签</strong>并重新参与损失计算。它也是通过设计<strong>损失权值</strong>$\lambda$来实现的，其设计如下：</p>
<script type="math/tex; mode=display">
  \lambda_i = \begin{cases}
  \frac{\log f(\boldsymbol{x})_i}{\log (1-f(\boldsymbol{x})_i)}, & \text{if} \ \boldsymbol{l}_i > R(t)\text{且该标签是缺失标签} \\
  1，& \text{otherwise}  \end{cases}</script><p>  最终计算的损失和LL-R一样，只是$\lambda_i$的设计不同。<br>  值得注意的是，当$\boldsymbol{l}_i$大于阈值$R(t)$且该标签是缺失标签时，我们将$\lambda_i$设为$\frac{\log f(\boldsymbol{x})_i}{\log (1-f(\boldsymbol{x})_i)}$，即临时修正该标签（直接从0暂时改为1）并重新参与损失计算，具体推导如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{L} = \frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \lambda_i \cdot \boldsymbol{l}_i</script><p>具体计算负标签的$\boldsymbol{l}_i$可以获得如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{L} = \frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \color{red}\lambda_i \cdot \text{BCE}_{\text{neg}}(f(\boldsymbol{x})_i,0)</script><p>具体计算$\text{BCE}_{\text{neg}}$，并且代入$\lambda_i=\frac{\log f(\boldsymbol{x})_i}{\log (1-f(\boldsymbol{x})_i)}$，可以得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{L} = \frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \frac{\log f(\boldsymbol{x})_i}{\log (1-f(\boldsymbol{x})_i)} \cdot -\log(1-f(\boldsymbol{x})_i)</script><p>即得到：</p>
<script type="math/tex; mode=display">
\boldsymbol{L} = \frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \cdot -\log f(\boldsymbol{x})_i=\frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \color{red}{1} \cdot \color{red}{\text{BCE}_{\text{pos}}(f(\boldsymbol{x})_i,1)}</script><p>这样，我们在计算损失时，就可以视作标签从负样本变成了正样本，损失权重$\lambda_i$从$\frac{\log f(\boldsymbol{x})_i}{\log (1-f(\boldsymbol{x})_i)}$变成了1，证明做到了临时修正。</p>
<h3 id="3-3-大损失标签修正-LL-Cp"><a href="#3-3-大损失标签修正-LL-Cp" class="headerlink" title="3.3 大损失标签修正(LL-Cp)"></a>3.3 大损失标签修正(LL-Cp)</h3><p>  该方法的核心思想是在训练过程中，修正大损失标签，即当$\boldsymbol{L}$大于阈值$R(t)$时，我们<strong>直接将该标签修正为正标签</strong>并进行下一轮训练。其设计如下：</p>
<p>  首先<span style="color: red;"><strong>这里阈值的设计有所不同</strong></span>，其将$R(t)$设定为一个固定值，并不随着训练轮次的增加而增加。</p>
<p>  其次对于获得的标签$\boldsymbol{y}^{AN}$，它将随时可能被修正为正标签，具体的修正方法如下：</p>
<script type="math/tex; mode=display">
  \boldsymbol{y}^{AN}_i = \begin{cases}
  1, & \text{if} \ \boldsymbol{l}_i > R(t)\text{且该标签是缺失标签} \\
  \boldsymbol{y}_i, & \text{otherwise}  \end{cases}</script><p>  修正后，直接计算交叉熵损失即可，即:</p>
<script type="math/tex; mode=display">
  \boldsymbol{L} = \frac{1}{N} \sum_{j=1}^{N} \sum_{i=1}^{K} \frac{1}{K} \boldsymbol{l}_i</script><p>  这样，我们可以通过修正大损失标签来防止模型记忆噪声标签,这是一个<strong>永久性的修正</strong>，而不是临时修正。</p>
<h2 id="⭐实验设定"><a href="#⭐实验设定" class="headerlink" title="⭐实验设定"></a>⭐实验设定</h2><h3 id="4-1-SOTA对比实验"><a href="#4-1-SOTA对比实验" class="headerlink" title="4.1 SOTA对比实验"></a>4.1 SOTA对比实验</h3><h4 id="✨人工创造不完整多标签数据集"><a href="#✨人工创造不完整多标签数据集" class="headerlink" title="✨人工创造不完整多标签数据集"></a>✨人工创造不完整多标签数据集</h4><p>  <strong>数据集说明</strong>：将现有的多标签数据集中的部分标签进行删除，从而构建不完整多标签数据集。具体来说，我们从现有的多标签数据集中只随机保留一个<strong>正标签</strong>，其余标签全部删除。所使用的数据集包括<span class="highlight-text"><strong>VOC 2012</strong></span>、<span class="highlight-text"><strong>MS COCO 2014</strong></span>、<span class="highlight-text"><strong>NUSWIDE</strong></span>和<span class="highlight-text"><strong>CUB</strong></span>。其中<span class="highlight-text"><strong>CUB</strong></span>是一个鸟类分类的数据集，<strong>注意</strong>这里我们不用于预测鸟类分类任务，而是用于预测鸟类的多标签特征例如颜色、形状等。</p>
<p>  <strong>数据处理</strong>：本文具体对数据进行了如下处理：</p>
<ol>
<li>使用相同的<strong>种子数</strong>来随机删除标签，以确保每个数据集的不完整多标签数据集是相同的。</li>
<li>测试模型选用在<span class="highlight-text"><strong>ImageNet</strong></span>上预训练的<span class="highlight-text"><strong>ResNet-50</strong></span>模型。</li>
<li>模型训练的Batch Size设置为 16。</li>
<li>训练时采取<strong>随机水平翻转</strong>进行数据增强。</li>
<li>所有输入图像的大小都调整为 448x448。</li>
<li>设定俩种训练策略，一种是<span class="highlight-normal"><strong>LinearInit</strong></span>，另一种是<span class="highlight-normal"><strong>End-to-end</strong></span>,其中LinearInit是在第一轮训练时，只训练模型最后一层，其余层的参数保持不变；End-to-end是直接训练整个模型。</li>
</ol>
<p>  <strong>对比方法</strong>：本文将提出的三种方法的俩种训练模式分别与以下方法进行对比：<span class="highlight-normal"><strong>Full label</strong></span>：直接使用完整标签训练模型；<span class="highlight-normal"><strong>Naive AN</strong></span>、<span class="highlight-normal"><strong>Weak AN(WAN)</strong></span>、Label Smoothing with AN<span class="highlight-normal"><strong>LSAN</strong></span>、和<span class="highlight-normal"><strong>ROLE</strong></span>。以上均是<strong>第二种解决流派</strong>的前沿方法；由于实验设定与第一种解决流派的前沿方法不同，因此不进行对比。</p>
<p>  <strong>实验结果</strong>：结果如下图。</p>
<p><div style="text-align:center;"><img src="/sucai/l1_f3.png" height="250"></div></p>
<h4 id="✨真实部分标注的数据集"><a href="#✨真实部分标注的数据集" class="headerlink" title="✨真实部分标注的数据集"></a>✨真实部分标注的数据集</h4><p>  <strong>数据集说明</strong>：这里使用了<span class="highlight-text"><strong>OpenImages V3</strong></span>数据集，该数据集包含了<strong>3.4M</strong>的训练图像、<strong>42K</strong>的验证图像和<strong>125K</strong>的测试图像，并且有<strong>5000</strong>个类别，且只有不到$1\%$的标签是被标注的。</p>
<p>  <strong>数据处理</strong>：本文具体对数据进行了如下处理：</p>
<ol>
<li>测试模型选用<span class="highlight-noraml"><strong>ImageNet-pretrained ResNet-101</strong></span>模型。</li>
<li>模型训练的Batch Size设置为 288，使用4个GPU。</li>
<li>所有输入图像的大小都调整为 224x224。</li>
<li>训练时采取<strong>随机水平翻转</strong>进行数据增强。</li>
<li>由于不同类别对应的图像数据的量是不同的，作者将数据集分为了5组，每一组有1000个类别，其中Group 1是数据量最小，Group 5是数据量最大。</li>
</ol>
<p>  <strong>对比方法</strong>：本文将提出的三种方法的俩种训练模式分别与以下方法进行对比：第一种流派技术<span class="highlight-normal"><strong>Naive IU</strong></span>、<span class="highlight-normal"><strong>Curriculum labeling</strong></span>、<span class="highlight-normal"><strong>IMCL</strong></span>。第二种流派技术<span class="highlight-normal"><strong>Naive AN</strong></span>、<span class="highlight-normal"><strong>WAN</strong></span>、<span class="highlight-normal"><strong>LSAN</strong></span>。</p>
<p>  <strong>实验结果</strong>：结果如下图。</p>
<figure style="text-align:center;">
  <img src="/sucai/l1_f4.png" height="220">
</figure>

<h3 id="4-2-消融实验"><a href="#4-2-消融实验" class="headerlink" title="4.2 消融实验"></a>4.2 消融实验</h3><p>消融实验主要分别对所提的<strong>损失修正方法</strong>和超参数<strong>$\Delta_{rel}$</strong>进行了实验，具体如下：</p>
<h4 id="✨损失修正消融实验"><a href="#✨损失修正消融实验" class="headerlink" title="✨损失修正消融实验"></a>✨损失修正消融实验</h4><p>  这里似乎也不完全可以称为消融实验，本实验主要是证明三种损失修正的模块的有效性。这里作者定义了其评价指标：<span class="highlight-text"><strong>修正准确率</strong></span>，其定义为<strong>模型成功修正或拒绝的标签数量</strong>与<strong>模型所有修正或拒绝的标签数量</strong>的比值。显然，修正准确率代表模型修正 <strong>假阴性标签</strong> 的能力。其实验结果如下：</p>
<p><figure style="text-align:center;">
  <img src="/sucai/l1_f5.png" height="250">
</figure><br>图中蓝色柱状线对应模型<strong>某一轮修正的总标签数</strong>，三种折线分别表示三种修正方法的<strong>修正准确率</strong>。从图中可以看出，三种修正方法都能保证<span style="color: red;"><strong>修正准确率高水平</strong></span>，说明三种方法确实对错误的标签进行的正确的处理。然而<span style="color: red;">随着训练轮次的增加，修正准确率逐渐下降</span>，这是因为随着训练轮次的增加，模型记忆噪声标签的能力增强，导致修正准确率下降。此外，三种方法中修正准确率最高的是<span class="highlight-text"><strong>LL-Cp</strong></span>，然后实际上分类性能最好的并不是LL-Cp，这是因为LL-Cp修正的标签是<strong>永久性的</strong>，一旦LL-Cp错误地修改了一个标签，它反而会降低模型的性能（这种方法过于激进）。<br><span class="yubox">可能读者可能还是疑惑为什么随着训练轮次的增加，修正准确率逐渐下降，难道我们的模型不就是为了解决模型后半期记忆噪声标签的问题吗？这里是因为只要没有百分百的错误标签被修正或拒绝，训练后期它一定会被记住，一旦这些错误标签影响了模型的表征，模型性能就会下降，性能一下降原本可能真阴性的样本可能也被误判为假阴性，导致修正准确率下降。</span></p>
<h4 id="✨超参数-Delta-rel-消融实验"><a href="#✨超参数-Delta-rel-消融实验" class="headerlink" title="✨超参数$\Delta_{rel}$消融实验"></a>✨超参数$\Delta_{rel}$消融实验</h4><p>  这里作者主要是对超参数$\Delta_{rel}$进行了实验，分别选取$\Delta_{rel}=0.1$到$10$进行的实验，统计LL-Ct不同超参数下的<strong>模型分类准确率</strong>。实验结果如下：</p>
<p><figure style="text-align:center;">
  <img src="/sucai/l1_f6.png" height="250">
</figure><br>可以看出此实验最佳的<span class="highlight-text"><strong>拒绝增长系数$\Delta_{rel}$</strong></span>是<strong>0.2</strong>。其中过小的$\Delta_{rel}$会导致模型在训练时没有<span style="color: red;">拒绝足够多的假阴性标签</span>，而过大的$\Delta_{rel}$会导致模型在训练时<span style="color: red;">错误地把一些真阴性标签拒绝</span>，从而影响模型的性能。</p>
<h3 id="4-3-可视化实验"><a href="#4-3-可视化实验" class="headerlink" title="4.3 可视化实验"></a>4.3 可视化实验</h3><p>这里主要就是举了几个具体的例子，效果如下：</p>
<p><figure style="text-align:center;">
  <img src="/sucai/l1_f7.png" height="250">
</figure><br>可以看出，所提方法可以有效地把<strong>把图片中缺失的视觉特征</strong>预测出来，第四张图是一个错误的例子，模型把卡车错误地预测为汽车，还莫名其妙预测出来个人。</p>
<h3 id="4-4-其他实验"><a href="#4-4-其他实验" class="headerlink" title="4.4 其他实验"></a>4.4 其他实验</h3><p>这里作者还进行了一些其他实验：可解释性实验、小数据训练实验，这里只介绍<strong>小数据训练实验</strong>，具体如下：<br>作者主要对<strong>COCO</strong>数据集进行了实验，根据原训练数据的$10\%$、$20\%$、$30\%$、$40\%$、$50\%$、$60\%$、$70\%$、$80\%$、$90\%$、$100\%$的标签进行训练，实验结果如下：</p>
<p><figure style="text-align:center;">
  <img src="/sucai/l1_f8.png" height="300">
</figure><br>可以看出，所提方法在<strong>小数据训练</strong>上的效果也是非常好的，超越其他现有方法，且准确率相当于<strong>全标签训练</strong>的效果。</p>
<h2 id="⭐笔者总结"><a href="#⭐笔者总结" class="headerlink" title="⭐笔者总结"></a>⭐笔者总结</h2><p>本文主要为笔者提供了一个新的研究领域：<strong>弱监督多标签学习（WSML）</strong>，这个问题在医学CT影像处理十分常见，它的方案也为我提供了一个可行的思路。</p>
<p>具体地，此文章沿用了“将未知标签视为负标签”的思路，将缺标签的多标签分类转化为了<strong>噪声多标签分类问题</strong>，并发现了噪声多标签分类中存在类似于<strong>噪声多分类问题</strong>的<strong>记忆效应</strong>。并提出了通过修正或拒绝大损失标签的方法来防止模型记忆噪声标签，这种方法简单且有效。</p>
<style>
  .highlight-text {
    background-color: #f0f8ff;         /* 设置背景色 */
    border-radius: 4px;                /* 设置圆角 */
    padding: 0 5px;                    /* 设置内边距，确保文本不与边框贴得太紧 */
    box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);  /* 添加轻微的阴影 */
    border: 2px solidrgb(0, 98, 133);         /* 添加边缘线，颜色为浅灰色，宽度为 2px */
  }
</style>

<style>
  .highlight-normal {
    background-color: #fff8dc;         /* 设置浅黄色背景色 */
    border-radius: 4px;                /* 设置圆角 */
    padding: 0 5px;                    /* 设置内边距，确保文本不与边框贴得太紧 */
    box-shadow: 1px 1px 3px rgba(0, 0, 0, 0.1);  /* 添加轻微的阴影 */
    border: 1px solid #f0e68c;         /* 添加浅黄色边缘线，颜色为浅黄色，宽度为 2px */
  }
</style>

<style>
  .bbox {
    background-color: #f0f8ff;        /* 设置背景色为浅黄色 */
    border-radius: 8px;               /* 设置圆角 */
    padding: 10px;                    /* 设置内边距 */
    box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* 添加轻微的阴影 */
    border: 2px solidrgb(126, 230, 206);        /* 设置浅黄色边框 */
    display: block;                   /* 确保框体显示为块元素，独占一行 */
    margin-top: 10px;                 /* 给框体添加顶部间距 */
    font-family: "Times New Roman", Times, serif; /* 设置字体为 Times New Roman */
  }
</style>


<style>
  .yubox {
    background-color:rgb(229, 234, 204);        /* 设置背景色为浅黄色 */
    border-radius: 8px;               /* 设置圆角 */
    padding: 10px;                    /* 设置内边距 */
    box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); /* 添加轻微的阴影 */
    border: 2px solidrgb(126, 230, 206);        /* 设置浅黄色边框 */
    display: block;                   /* 确保框体显示为块元素，独占一行 */
    margin-top: 10px;                 /* 给框体添加顶部间距 */
  }
</style>

<style>
  @keyframes scrollText {
    0% {
      transform: translateX(100%);
    }
    100% {
      transform: translateX(-100%);
    }
  }
</style>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">yuyu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/02/21/post1/">http://example.com/2025/02/21/post1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">yuyu</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">弱监督学习</a><a class="post-meta__tags" href="/tags/%E5%BC%B1%E6%A0%87%E7%AD%BE%E5%AD%A6%E4%B9%A0/">弱标签学习</a><a class="post-meta__tags" href="/tags/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%AD%A6%E4%B9%A0/">多标签学习</a></div><div class="post-share"><div class="social-share" data-image="https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/02/19/hello-world/" title="Hello World"><img class="cover" src="https://picx.zhimg.com/v2-31af717d5ebecb7d19dd56b65383af7f_1440w.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment </div></div></div></a><a class="pagination-related" href="/2025/02/28/post2/" title="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification"><img class="cover" src="https://pic4.zhimg.com/v2-94b8e424e313f6351b185f8f572ef271_r.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification</div></div><div class="info-2"><div class="info-item-1">        ⭐论文信息   文献类型：会议论文(CCFB)   发表刊物：MICCAI   发表时间：2024   发表单位：纽约州立大学石溪分校   1.1 拟解决的科学问题✨ 本论文旨在解决Transformer在医学图像分类任务中的应用问题，主要解决了Transformer在医学图像分类任务中特征提取不佳和不能很好地传播有效的信息的问题。✨ 本文属于医学图像分类领域，我还将其归为多尺度融合领域，具体涉及到局部全局特征融合领域。 ⭐论文背景2.1 基本背景和前提技术✨ 多尺度融合：多尺度融合是指将不同尺度的特征进行融合，以提高特征的表达能力，在图像特征提取中，多尺度融合常表明图像的不同分辨率的特征融合，例如在CNN中设计的多尺度卷积核，卷积核的大小不同，意味着卷积核能够提取不同尺度的特征：越小的卷积核提取的是细节特征，越大的卷积核提取的是全局特征，如何将这些各种尺度的特征融合起来，学术界称之为多尺度融合任务。 ✨...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/16/post4/" title="【技术篇】BCE or CE? 多标签 还是 多分类？"><img class="cover" src="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-16</div><div class="info-item-2">【技术篇】BCE or CE? 多标签 还是 多分类？</div></div><div class="info-2"><div class="info-item-1">我们在选择损失函数的时候，经常会遇到这样的问题，是选择二元交叉熵（Binary Cross Entropy，BCE），还是选择交叉熵（Cross Entropy，CE）？或许有人简单的认为，二分类问题选择BCE，多分类问题选择CE。那是不是BCE就只能用于二分类问题呢？那么到底有什么细节需要注意呢？本文将为你一一解答。 读完本文你将明白如下：  BCE不仅适用于二分类问题，还适用于多标签问题； CE是不会注意负标签的损失的，而BCE会计算负标签的损失；（BCE不是简单的二元CE） BCE 和 CE 在torch中计算log的底数取e，即ln； CE 在torch中输入接收为logits，而BCE接收为概率值； BCE 和 CE 在torch中还有一个参数reduction，用于控制损失的计算方式； CE 在torch接受的标签是向量而不是矩阵。  ⭐二分类、多分类、多标签二分类问题在正式描述 BCE 和 CE 之前，我们先来了解一下二分类、多分类和多标签的概念。 二分类任务很简单，就是将输入图片分成俩类，即True 和...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/yu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">yuyu</div><div class="author-info-description">欢迎来到yuyu的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/gityuyuhub" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:2743342512@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">如你所见，这是我的博客，相信在这些文章中，你会找到你想要的。This is Yuyu</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">
  
    本论文在论文分享会中分享，点击查看论文PPT 👆
  
</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E8%AE%BA%E6%96%87%E4%BF%A1%E6%81%AF"><span class="toc-number">2.</span> <span class="toc-text">⭐论文信息</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%8B%9F%E8%A7%A3%E5%86%B3%E7%9A%84%E7%A7%91%E5%AD%A6%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 拟解决的科学问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E8%AE%BA%E6%96%87%E8%83%8C%E6%99%AF"><span class="toc-number">3.</span> <span class="toc-text">⭐论文背景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%9F%BA%E6%9C%AC%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 基本背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%8C%96%E5%9D%91"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 挖坑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E4%B8%80%E6%AE%B5%E8%AF%9D%E4%BB%8B%E7%BB%8D%E6%8A%80%E6%9C%AF"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 一段话介绍技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.4.</span> <span class="toc-text">2.4 相关工作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E8%AE%BA%E6%96%87%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">⭐论文方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%A4%A7%E6%8D%9F%E5%A4%B1%E6%A0%87%E7%AD%BE%E6%8B%92%E7%BB%9D-LL-R"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 大损失标签拒绝(LL-R)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%88%E5%80%BC%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.1.1.</span> <span class="toc-text">阈值设计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E6%9D%83%E5%80%BC%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.1.2.</span> <span class="toc-text">损失权值设计</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%A4%A7%E6%8D%9F%E5%A4%B1%E6%A0%87%E7%AD%BE%E4%B8%B4%E6%97%B6%E4%BF%AE%E6%AD%A3-LL-Ct"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 大损失标签临时修正(LL-Ct)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%A4%A7%E6%8D%9F%E5%A4%B1%E6%A0%87%E7%AD%BE%E4%BF%AE%E6%AD%A3-LL-Cp"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 大损失标签修正(LL-Cp)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%AE%9A"><span class="toc-number">5.</span> <span class="toc-text">⭐实验设定</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-SOTA%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 SOTA对比实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%9C%A8%E4%BA%BA%E5%B7%A5%E5%88%9B%E9%80%A0%E4%B8%8D%E5%AE%8C%E6%95%B4%E5%A4%9A%E6%A0%87%E7%AD%BE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.1.1.</span> <span class="toc-text">✨人工创造不完整多标签数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%9C%A8%E7%9C%9F%E5%AE%9E%E9%83%A8%E5%88%86%E6%A0%87%E6%B3%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.1.2.</span> <span class="toc-text">✨真实部分标注的数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 消融实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%9C%A8%E6%8D%9F%E5%A4%B1%E4%BF%AE%E6%AD%A3%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.2.1.</span> <span class="toc-text">✨损失修正消融实验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%9C%A8%E8%B6%85%E5%8F%82%E6%95%B0-Delta-rel-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.2.2.</span> <span class="toc-text">✨超参数$\Delta_{rel}$消融实验</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 可视化实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%85%B6%E4%BB%96%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.4.</span> <span class="toc-text">4.4 其他实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%AD%90%E7%AC%94%E8%80%85%E6%80%BB%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">⭐笔者总结</span></a></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/21/post1/" title="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification"><img src="https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification"></a><div class="content"><a class="title" href="/2025/02/21/post1/" title="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification">【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification</a><time datetime="2025-02-20T16:54:42.000Z" title="发表于 2025-02-21 00:54:42">2025-02-21</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/04/21/post5/" title="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model"><img src="https://pic1.zhimg.com/v2-9efa9909ceaf3f5874aa9de8c290610e_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model"/></a><div class="content"><a class="title" href="/2025/04/21/post5/" title="【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model">【论文阅读】：Med-2E3：A 2D-Enhanced 3D Medical Multimodal Large Language Model</a><time datetime="2025-04-20T17:57:17.000Z" title="发表于 2025-04-21 01:57:17">2025-04-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/16/post4/" title="【技术篇】BCE or CE? 多标签 还是 多分类？"><img src="https://pic1.zhimg.com/v2-c49dda70ee82730ec6f2c46f58df84e4_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【技术篇】BCE or CE? 多标签 还是 多分类？"/></a><div class="content"><a class="title" href="/2025/03/16/post4/" title="【技术篇】BCE or CE? 多标签 还是 多分类？">【技术篇】BCE or CE? 多标签 还是 多分类？</a><time datetime="2025-03-15T16:54:00.000Z" title="发表于 2025-03-16 00:54:00">2025-03-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/04/post3/" title="【技术篇】transformer解析"><img src="https://pica.zhimg.com/v2-fa99c36ab8d889eb688fd3b67b4fc448_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【技术篇】transformer解析"/></a><div class="content"><a class="title" href="/2025/03/04/post3/" title="【技术篇】transformer解析">【技术篇】transformer解析</a><time datetime="2025-03-03T16:58:17.000Z" title="发表于 2025-03-04 00:58:17">2025-03-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/post2/" title="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification"><img src="https://pic4.zhimg.com/v2-94b8e424e313f6351b185f8f572ef271_r.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification"/></a><div class="content"><a class="title" href="/2025/02/28/post2/" title="【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification">【论文阅读】：Med-Former： A Transformer-based Architecture for Medical Image Classification</a><time datetime="2025-02-27T16:49:17.000Z" title="发表于 2025-02-28 00:49:17">2025-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/21/post1/" title="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification"><img src="https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification"/></a><div class="content"><a class="title" href="/2025/02/21/post1/" title="【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification">【论文阅读】：Large Loss Matters in Weakly Supervised Multi-Label Classification</a><time datetime="2025-02-20T16:54:42.000Z" title="发表于 2025-02-21 00:54:42">2025-02-21</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://pica.zhimg.com/v2-f71d85e2562138561950f98bea051964_1440w.jpg);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By yuyu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23lienmr7axdYseqeC',
      clientSecret: 'b7f1b4e555108e1113715eb05f7527c6d1ad667d',
      repo: 'gityuyuhub.github.io',
      owner: 'gityuyuhub',
      admin: ['gityuyuhub'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'b3019ecec0c64ab3bb8424ee1452326f'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>